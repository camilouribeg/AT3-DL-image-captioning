{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camilouribeg/AT3-DL-image-captioning/blob/main/Amal_Experiment_(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "**Assessment 3: Image Captioning Project**\n",
        "\n",
        "**94691 – Deep Learning Autumn 2025**\n",
        "\n",
        "Experiment Amal Almaawadh - 24901098"
      ],
      "metadata": {
        "id": "THC4PVMDQbLe"
      },
      "id": "THC4PVMDQbLe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Colab Settings"
      ],
      "metadata": {
        "id": "IHh7m-RrPXoQ"
      },
      "id": "IHh7m-RrPXoQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaBM6wFCPV3w",
        "outputId": "d6e71fb1-3077-4061-c3f9-a7c558c3048c"
      },
      "id": "RaBM6wFCPV3w",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change working directory\n",
        "import os\n",
        "project_path = '/content/drive/MyDrive/Colab Notebooks/DL_Assignment3'\n",
        "os.chdir(project_path)"
      ],
      "metadata": {
        "id": "ziNIc_4XP7AO"
      },
      "id": "ziNIc_4XP7AO",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_HLi1SBRhlF",
        "outputId": "5fea5a47-6f5b-458b-f8cc-88f5c4474f58"
      },
      "id": "s_HLi1SBRhlF",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;36m'Colab Notebooks'\u001b[0m@   \u001b[01;34mdata\u001b[0m/   \u001b[01;34mnotebooks\u001b[0m/   README.md   requirements.txt   \u001b[01;34mutils\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d812846f",
      "metadata": {
        "id": "d812846f"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bcc0c770",
      "metadata": {
        "id": "bcc0c770"
      },
      "outputs": [],
      "source": [
        "# Project Imports\n",
        "import sys\n",
        "sys.path.append(\"..\")   # so `utils/` is importable\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from utils.dataloader import get_transforms, load_split_ids, build_caption_dataset\n",
        "from utils.caption_dataset import CaptionDataset\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Add utils to path to import project modules\n",
        "import sys\n",
        "sys.path.append('utils')"
      ],
      "metadata": {
        "id": "YHk84EShRsjZ"
      },
      "id": "YHk84EShRsjZ",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.dataloader import get_transforms, load_split_ids, build_caption_dataset\n",
        "from utils.caption_dataset import CaptionDataset"
      ],
      "metadata": {
        "id": "6BPCTTCCR4vh"
      },
      "id": "6BPCTTCCR4vh",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm if the class was imported\n",
        "print(CaptionDataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwaOqRB3Zp3v",
        "outputId": "9437fb00-63d3-4b37-e3b7-bbcd46674c2b"
      },
      "id": "gwaOqRB3Zp3v",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'utils.caption_dataset.CaptionDataset'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Datasets"
      ],
      "metadata": {
        "id": "UZr6ML-kSM5p"
      },
      "id": "UZr6ML-kSM5p"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "92fd60f7",
      "metadata": {
        "id": "92fd60f7"
      },
      "outputs": [],
      "source": [
        "# Load vocabulary\n",
        "\n",
        "processed_path = os.path.join(project_path, 'data', 'processed')\n",
        "\n",
        "# Load vocabulary\n",
        "with open(os.path.join(processed_path, 'word2idx.json'), 'r') as f:\n",
        "    word2idx = json.load(f)\n",
        "\n",
        "# Load image-caption sequences\n",
        "with open(os.path.join(processed_path, 'image_caption_seqs.pkl'), 'rb') as f:\n",
        "    image_caption_seqs = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c4b33923",
      "metadata": {
        "id": "c4b33923"
      },
      "outputs": [],
      "source": [
        "# Load official splits\n",
        "\n",
        "train_ids = load_split_ids(os.path.join(project_path, 'data', 'Flickr8k_text', 'Flickr_8k.trainImages.txt'))\n",
        "val_ids   = load_split_ids(os.path.join(project_path, 'data', 'Flickr8k_text', 'Flickr_8k.devImages.txt'))\n",
        "test_ids  = load_split_ids(os.path.join(project_path, 'data', 'Flickr8k_text', 'Flickr_8k.testImages.txt'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocesing (Just Images)"
      ],
      "metadata": {
        "id": "-xqexvqYVvWX"
      },
      "id": "-xqexvqYVvWX"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "65931e1a",
      "metadata": {
        "id": "65931e1a"
      },
      "outputs": [],
      "source": [
        "# Set image folder path\n",
        "\n",
        "image_folder = os.path.join(project_path, 'data', 'Flicker8k_Dataset')\n",
        "\n",
        "# Define transforms\n",
        "transform_train = get_transforms(\"train\")\n",
        "transform_val   = get_transforms(\"val\")\n",
        "\n",
        "# Build datasets using shared util function\n",
        "train_dataset = build_caption_dataset(train_ids, image_caption_seqs, word2idx, image_folder, transform_train)\n",
        "val_dataset   = build_caption_dataset(val_ids, image_caption_seqs, word2idx, image_folder, transform_val)\n",
        "test_dataset  = build_caption_dataset(test_ids, image_caption_seqs, word2idx, image_folder, transform_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment Architecture"
      ],
      "metadata": {
        "id": "NVXra222Z-We"
      },
      "id": "NVXra222Z-We"
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder: ResNet-101 + Flatten to (B, N, C)\n",
        "class EncoderResNet101(nn.Module):\n",
        "    def __init__(self, encoded_size=14, fine_tune=False):\n",
        "        super().__init__()\n",
        "        # 1) Load pretrained ResNet-101\n",
        "        resnet = models.resnet101(pretrained=True)\n",
        "        # 2) Drop the avgpool & fc layers\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.backbone = nn.Sequential(*modules)\n",
        "        # 3) Our own adaptive pooling\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_size, encoded_size))\n",
        "        # 4) Freeze if not fine-tuning\n",
        "        if not fine_tune:\n",
        "            for p in self.backbone.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "    def forward(self, images):\n",
        "        # images: (B, 3, H, W)\n",
        "        x = self.backbone(images)               # → (B, 2048, Hf, Wf)\n",
        "        x = self.adaptive_pool(x)               # → (B, 2048, enc, enc)\n",
        "        B, C, H, W = x.size()\n",
        "        return x.view(B, C, H * W).permute(0, 2, 1)  # → (B, N=H*W, C)"
      ],
      "metadata": {
        "id": "Bl6p1gpxikBI"
      },
      "id": "Bl6p1gpxikBI",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder: GRU + Bahdanau Attention\n",
        "class DecoderGRUWithBahdanau(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim,\n",
        "                 encoder_dim=512, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        # attention: score = v^T tanh(W_h h + W_e e)\n",
        "        self.attn      = nn.Linear(hidden_dim + encoder_dim, hidden_dim)\n",
        "        self.attn_v    = nn.Linear(hidden_dim, 1, bias=False)\n",
        "        self.gru       = nn.GRU(embed_dim + encoder_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_out    = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.dropout   = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, encoder_seq, captions):\n",
        "        \"\"\"\n",
        "        encoder_seq: (B, N, encoder_dim)\n",
        "        captions:    (B, T)\n",
        "        \"\"\"\n",
        "        B, N, _ = encoder_seq.size()\n",
        "        T = captions.size(1)\n",
        "        embeddings = self.embed(captions)            # (B, T, E)\n",
        "        hidden = torch.zeros(1, B, self.gru.hidden_size,\n",
        "                             device=encoder_seq.device)\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(T):\n",
        "            # ——— Bahdanau Attention ———\n",
        "            # expand hidden to (B, N, H)\n",
        "            h_exp = hidden.permute(1, 0, 2).expand(B, N, -1)\n",
        "            # concat [h_exp, encoder_seq] → (B, N, H+E)\n",
        "            attn_in = torch.cat([h_exp, encoder_seq], dim=2)\n",
        "            energy  = torch.tanh(self.attn(attn_in))       # (B, N, H)\n",
        "            scores  = self.attn_v(energy).squeeze(2)       # (B, N)\n",
        "            alpha   = torch.softmax(scores, dim=1).unsqueeze(2)  # (B, N, 1)\n",
        "            # context vector\n",
        "            context = (encoder_seq * alpha).sum(dim=1)     # (B, E)\n",
        "\n",
        "            # ——— GRU step ———\n",
        "            inp = torch.cat([embeddings[:, t, :], context], dim=1).unsqueeze(1)  # (B,1,E+E)\n",
        "            out, hidden = self.gru(inp, hidden)           # out: (B,1,H)\n",
        "            out = self.dropout(out.squeeze(1))            # (B, H)\n",
        "            out = self.fc_out(out)                        # (B, V)\n",
        "            outputs.append(out)\n",
        "\n",
        "        # stack → (B, T, V)\n",
        "        return torch.stack(outputs, dim=1)\n"
      ],
      "metadata": {
        "id": "TQXnFzmNqKYF"
      },
      "id": "TQXnFzmNqKYF",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap encoder+decoder in a single module\n",
        "class CaptioningModel(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        enc_seq = self.encoder(images)        # (B, N, C_e)\n",
        "        return self.decoder(enc_seq, captions)  # (B, T, V)\n"
      ],
      "metadata": {
        "id": "zhWIpMxQqRVh"
      },
      "id": "zhWIpMxQqRVh",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for training the model\n",
        "\n",
        "def train_model(model, train_dataset, val_dataset, word2idx,\n",
        "                device='cuda', batch_size=32, epochs=20,\n",
        "                patience=3, lr=1e-4):\n",
        "    pad_idx = word2idx['<pad>']\n",
        "    vocab_size = len(word2idx)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', patience=2, factor=0.5\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=4,       # parallel data loading\n",
        "    pin_memory=True      # faster host→GPU copies\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    wait = 0\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # ——— Training ———\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for images, captions, _ in tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\"):\n",
        "            # ----- CLAMP ANY INVALID TOKEN IDS -----\n",
        "            max_id = captions.max().item()\n",
        "            if max_id >= vocab_size:\n",
        "                # replace all OOB ids with the pad token\n",
        "                captions = torch.where(\n",
        "                    captions >= vocab_size,\n",
        "                    torch.full_like(captions, pad_idx),\n",
        "                    captions\n",
        "                )\n",
        "            # ---------------------------------------\n",
        "            images   = images.to(device,   non_blocking=True)\n",
        "            captions = captions.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, captions[:, :-1])\n",
        "            loss = criterion(\n",
        "                outputs.reshape(-1, outputs.size(-1)),\n",
        "                captions[:, 1:].reshape(-1)\n",
        "            )\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        # ——— Validation ———\n",
        "        model.eval()\n",
        "        val_losses, refs, hyps = [], [], []\n",
        "        for images, captions, _ in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\"):\n",
        "            # ----- CLAMP HERE AS WELL -----\n",
        "            max_id = captions.max().item()\n",
        "            if max_id >= vocab_size:\n",
        "                captions = torch.where(\n",
        "                    captions >= vocab_size,\n",
        "                    torch.full_like(captions, pad_idx),\n",
        "                    captions\n",
        "                )\n",
        "            # --------------------------------\n",
        "            images, captions = images.to(device), captions.to(device)\n",
        "\n",
        "            outputs = model(images, captions[:, :-1])\n",
        "            loss = criterion(\n",
        "                outputs.reshape(-1, outputs.size(-1)),\n",
        "                captions[:, 1:].reshape(-1)\n",
        "            )\n",
        "            val_losses.append(loss.item())\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=2)\n",
        "            for ref, pred in zip(captions, preds):\n",
        "                ref_tokens  = [w for w in ref.tolist()\n",
        "                               if w not in {pad_idx, word2idx['<start>'], word2idx['<end>']}]\n",
        "                pred_tokens = [w for w in pred.tolist()\n",
        "                               if w not in {pad_idx, word2idx['<start>'], word2idx['<end>']}]\n",
        "                refs.append([ref_tokens])\n",
        "                hyps.append(pred_tokens)\n",
        "\n",
        "        avg_train = np.mean(train_losses)\n",
        "        avg_val   = np.mean(val_losses)\n",
        "        scheduler.step(avg_val)\n",
        "\n",
        "        bleu1 = corpus_bleu(refs, hyps, weights=(1,0,0,0))\n",
        "        bleu2 = corpus_bleu(refs, hyps, weights=(0.5,0.5,0,0))\n",
        "        bleu3 = corpus_bleu(refs, hyps, weights=(0.33,0.33,0.33,0))\n",
        "        bleu4 = corpus_bleu(refs, hyps, weights=(0.25,0.25,0.25,0.25))\n",
        "\n",
        "        print(f\"\\nEpoch {epoch}: Train Loss={avg_train:.4f}, Val Loss={avg_val:.4f}\")\n",
        "        print(f\"BLEU-1={bleu1:.4f}, BLEU-2={bleu2:.4f}, BLEU-3={bleu3:.4f}, BLEU-4={bleu4:.4f}\")\n",
        "\n",
        "        if avg_val < best_val_loss:\n",
        "            best_val_loss = avg_val\n",
        "            wait = 0\n",
        "            torch.save(model.state_dict(), \"best_resnet18_gru_bahdanau.pth\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "GeF2UqQZqULD"
      },
      "id": "GeF2UqQZqULD",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL6IA4aqxKEG",
        "outputId": "9dcbde7c-9b86-42ef-9452-a4ac2769aa75"
      },
      "id": "WL6IA4aqxKEG",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Instantiate & Train with ResNet-101 encoder (CNN frozen for speed)\n",
        "encoder = EncoderResNet101(encoded_size=7, fine_tune=False).to(device)\n",
        "decoder = DecoderGRUWithBahdanau(\n",
        "    vocab_size=len(word2idx),\n",
        "    embed_dim=256,\n",
        "    hidden_dim=512,\n",
        "    encoder_dim=2048,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "model = CaptioningModel(encoder, decoder).to(device)\n",
        "print(\"Encoded patches per image:\", 7*7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3tsyn8lHSjs",
        "outputId": "e9626d78-af73-45e6-d776-e29689cd8a17"
      },
      "id": "D3tsyn8lHSjs",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:00<00:00, 230MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded patches per image: 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = train_model(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    word2idx=word2idx,\n",
        "    device=device,\n",
        "    batch_size=32,\n",
        "    epochs=15,\n",
        "    patience=3,\n",
        "    lr=1e-4\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ndzozgfqg4W",
        "outputId": "54bd9bda-c5c3-494d-85c2-b0766947a8e9"
      },
      "id": "0ndzozgfqg4W",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Train]: 100%|██████████| 937/937 [19:56<00:00,  1.28s/it]\n",
            "Epoch 1 [Val]: 100%|██████████| 157/157 [03:14<00:00,  1.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: Train Loss=4.5145, Val Loss=3.8761\n",
            "BLEU-1=0.3188, BLEU-2=0.1590, BLEU-3=0.0803, BLEU-4=0.0413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Train]: 100%|██████████| 937/937 [01:44<00:00,  9.00it/s]\n",
            "Epoch 2 [Val]: 100%|██████████| 157/157 [00:12<00:00, 12.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: Train Loss=3.7066, Val Loss=3.5160\n",
            "BLEU-1=0.3453, BLEU-2=0.1841, BLEU-3=0.1007, BLEU-4=0.0545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 [Train]: 100%|██████████| 937/937 [01:46<00:00,  8.79it/s]\n",
            "Epoch 3 [Val]: 100%|██████████| 157/157 [00:12<00:00, 12.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: Train Loss=3.4134, Val Loss=3.3336\n",
            "BLEU-1=0.3561, BLEU-2=0.1944, BLEU-3=0.1099, BLEU-4=0.0610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 [Train]: 100%|██████████| 937/937 [01:46<00:00,  8.80it/s]\n",
            "Epoch 4 [Val]: 100%|██████████| 157/157 [00:12<00:00, 12.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: Train Loss=3.2311, Val Loss=3.2171\n",
            "BLEU-1=0.3635, BLEU-2=0.2030, BLEU-3=0.1163, BLEU-4=0.0648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 [Train]: 100%|██████████| 937/937 [01:46<00:00,  8.81it/s]\n",
            "Epoch 5 [Val]: 100%|██████████| 157/157 [00:12<00:00, 12.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5: Train Loss=3.0990, Val Loss=3.1326\n",
            "BLEU-1=0.3727, BLEU-2=0.2088, BLEU-3=0.1199, BLEU-4=0.0669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 [Train]: 100%|██████████| 937/937 [01:46<00:00,  8.77it/s]\n",
            "Epoch 6 [Val]: 100%|██████████| 157/157 [00:12<00:00, 12.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6: Train Loss=2.9974, Val Loss=3.0653\n",
            "BLEU-1=0.3829, BLEU-2=0.2185, BLEU-3=0.1279, BLEU-4=0.0728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 [Train]: 100%|██████████| 937/937 [01:47<00:00,  8.74it/s]\n",
            "Epoch 7 [Val]: 100%|██████████| 157/157 [00:12<00:00, 12.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7: Train Loss=2.9112, Val Loss=3.0180\n",
            "BLEU-1=0.3872, BLEU-2=0.2183, BLEU-3=0.1261, BLEU-4=0.0711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 [Train]: 100%|██████████| 937/937 [01:46<00:00,  8.78it/s]\n",
            "Epoch 8 [Val]: 100%|██████████| 157/157 [00:12<00:00, 12.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8: Train Loss=2.8402, Val Loss=2.9824\n",
            "BLEU-1=0.3905, BLEU-2=0.2210, BLEU-3=0.1280, BLEU-4=0.0728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 [Train]: 100%|██████████| 937/937 [01:46<00:00,  8.79it/s]\n",
            "Epoch 9 [Val]: 100%|██████████| 157/157 [00:12<00:00, 12.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9: Train Loss=2.7777, Val Loss=2.9503\n",
            "BLEU-1=0.3911, BLEU-2=0.2228, BLEU-3=0.1305, BLEU-4=0.0750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 [Train]: 100%|██████████| 937/937 [01:46<00:00,  8.78it/s]\n",
            "Epoch 10 [Val]: 100%|██████████| 157/157 [00:12<00:00, 12.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10: Train Loss=2.7237, Val Loss=2.9232\n",
            "BLEU-1=0.3944, BLEU-2=0.2260, BLEU-3=0.1332, BLEU-4=0.0771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 [Train]: 100%|██████████| 937/937 [01:46<00:00,  8.77it/s]\n",
            "Epoch 11 [Val]: 100%|██████████| 157/157 [00:12<00:00, 12.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11: Train Loss=2.6740, Val Loss=2.9019\n",
            "BLEU-1=0.3960, BLEU-2=0.2274, BLEU-3=0.1342, BLEU-4=0.0770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 [Train]: 100%|██████████| 937/937 [01:46<00:00,  8.78it/s]\n",
            "Epoch 12 [Val]: 100%|██████████| 157/157 [00:12<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12: Train Loss=2.6294, Val Loss=2.8898\n",
            "BLEU-1=0.4025, BLEU-2=0.2314, BLEU-3=0.1370, BLEU-4=0.0793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 [Train]: 100%|██████████| 937/937 [01:46<00:00,  8.79it/s]\n",
            "Epoch 13 [Val]: 100%|██████████| 157/157 [00:13<00:00, 11.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13: Train Loss=2.5888, Val Loss=2.8681\n",
            "BLEU-1=0.4024, BLEU-2=0.2330, BLEU-3=0.1394, BLEU-4=0.0814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 [Train]: 100%|██████████| 937/937 [01:46<00:00,  8.81it/s]\n",
            "Epoch 14 [Val]: 100%|██████████| 157/157 [00:12<00:00, 12.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14: Train Loss=2.5480, Val Loss=2.8556\n",
            "BLEU-1=0.4076, BLEU-2=0.2363, BLEU-3=0.1415, BLEU-4=0.0817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 [Train]: 100%|██████████| 937/937 [01:46<00:00,  8.76it/s]\n",
            "Epoch 15 [Val]: 100%|██████████| 157/157 [00:12<00:00, 12.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15: Train Loss=2.5119, Val Loss=2.8445\n",
            "BLEU-1=0.4058, BLEU-2=0.2345, BLEU-3=0.1393, BLEU-4=0.0803\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model weights and full model\n",
        "save_path = os.path.join(project_path, 'data')\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "torch.save(model.state_dict(), os.path.join(save_path, 'experiment_Amal_model3.pth'))\n",
        "torch.save(model, os.path.join(save_path, 'experiment_Amal_model3_full.pth'))"
      ],
      "metadata": {
        "id": "4fiZsatMlztr"
      },
      "id": "4fiZsatMlztr",
      "execution_count": 18,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}