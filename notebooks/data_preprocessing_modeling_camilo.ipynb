{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9606eab0",
   "metadata": {},
   "source": [
    "# Image Captioning - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab03a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import json\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import sys \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b65872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n",
      "1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\n",
      "1000268201_693b08cb0e.jpg#3\tA little girl climbing the stairs to her playhouse .\n",
      "1000268201_693b08cb0e.jpg#4\tA little girl in a pink dress going into a wooden cabin .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to captions\n",
    "captions_file = Path(\"../data/Flickr8k_text/Flickr8k.token.txt\")\n",
    "\n",
    "# Load all lines\n",
    "with open(captions_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Preview a few lines\n",
    "for line in lines[:5]:\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afff940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 1000268201_693b08cb0e.jpg\n",
      "Captions:\n",
      "- A child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "- A girl going into a wooden building .\n",
      "- A little girl climbing into a wooden playhouse .\n",
      "- A little girl climbing the stairs to her playhouse .\n",
      "- A little girl in a pink dress going into a wooden cabin .\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary with lists\n",
    "image_captions = defaultdict(list)\n",
    "\n",
    "# Parse each line\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if '\\t' in line:\n",
    "        img_id_with_index, caption = line.split('\\t')\n",
    "        img_id = img_id_with_index.split('#')[0]\n",
    "        image_captions[img_id].append(caption)\n",
    "\n",
    "# Preview example\n",
    "example_key = list(image_captions.keys())[0]\n",
    "print(f\"Image: {example_key}\")\n",
    "print(\"Captions:\")\n",
    "for cap in image_captions[example_key]:\n",
    "    print(f\"- {cap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc65b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_caption(caption):\n",
    "    # Convert to lowercase\n",
    "    caption = caption.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    caption = caption.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove numbers (optional)\n",
    "    caption = re.sub(r'\\d+', '', caption)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    caption = caption.strip()\n",
    "    \n",
    "    # Add special tokens\n",
    "    caption = f\"<start> {caption} <end>\"\n",
    "    \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4611abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned captions for 1000268201_693b08cb0e.jpg:\n",
      "- <start> a child in a pink dress is climbing up a set of stairs in an entry way <end>\n",
      "- <start> a girl going into a wooden building <end>\n",
      "- <start> a little girl climbing into a wooden playhouse <end>\n",
      "- <start> a little girl climbing the stairs to her playhouse <end>\n",
      "- <start> a little girl in a pink dress going into a wooden cabin <end>\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning to all captions\n",
    "for img_id in image_captions:\n",
    "    cleaned = [clean_caption(c) for c in image_captions[img_id]]\n",
    "    image_captions[img_id] = cleaned\n",
    "\n",
    "# Preview cleaned captions\n",
    "print(f\"Cleaned captions for {example_key}:\")\n",
    "for cap in image_captions[example_key]:\n",
    "    print(f\"- {cap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07215077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab (after filtering): 2990\n",
      "Sample mapping: 'girl' → 1055\n"
     ]
    }
   ],
   "source": [
    "# Flatten all captions into a single list of words\n",
    "all_captions = []\n",
    "for captions in image_captions.values():\n",
    "    for cap in captions:\n",
    "        all_captions.extend(cap.split())\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(all_captions)\n",
    "\n",
    "# Minimum word frequency threshold\n",
    "min_word_freq = 5\n",
    "\n",
    "# Filter out rare words\n",
    "words = [w for w in word_freq if word_freq[w] >= min_word_freq]\n",
    "\n",
    "# Special tokens\n",
    "special_tokens = ['<pad>', '<start>', '<end>', '<unk>']\n",
    "\n",
    "# Final vocabulary\n",
    "vocab = special_tokens + sorted(words)\n",
    "\n",
    "# Build mappings\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "print(f\"Total words in vocab (after filtering): {len(vocab)}\")\n",
    "print(f\"Sample mapping: 'girl' → {word2idx.get('girl')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cad79b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed captions for 1000268201_693b08cb0e.jpg:\n",
      "[5, 6, 496, 1278, 6, 1875, 768, 1311, 530, 2821, 6, 2220, 1695, 2468, 1278, 54, 3, 2902, 4]\n",
      "[5, 6, 1055, 1076, 1309, 6, 2957, 354, 4]\n",
      "[5, 6, 1479, 1055, 530, 1309, 6, 2957, 1906, 4]\n",
      "[5, 6, 1479, 1055, 530, 2657, 2468, 2699, 1198, 1906, 4]\n",
      "[5, 6, 1479, 1055, 1278, 6, 1875, 768, 1076, 1309, 6, 2957, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Convert a caption into a list of word indices\n",
    "def caption_to_indices(caption, word2idx):\n",
    "    return [word2idx.get(word, word2idx['<unk>']) for word in caption.split()]\n",
    "\n",
    "# Store caption sequences in a new dictionary\n",
    "image_caption_seqs = {}\n",
    "\n",
    "for img_id, captions in image_captions.items():\n",
    "    image_caption_seqs[img_id] = [caption_to_indices(cap, word2idx) for cap in captions]\n",
    "\n",
    "# Preview example\n",
    "print(f\"Indexed captions for {example_key}:\")\n",
    "for seq in image_caption_seqs[example_key]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60663358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded shape: torch.Size([40460, 37])\n",
      "First padded caption: tensor([   5,    6,  496, 1278,    6, 1875,  768, 1311,  530, 2821,    6, 2220,\n",
      "        1695, 2468, 1278,   54,    3, 2902,    4,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0])\n",
      "Original length: 19\n"
     ]
    }
   ],
   "source": [
    "# Convert lists of token IDs into tensors\n",
    "all_seqs = []\n",
    "for img_id, caption_list in image_caption_seqs.items():\n",
    "    for seq in caption_list:\n",
    "        all_seqs.append(torch.tensor(seq, dtype=torch.long))\n",
    "\n",
    "# Pad all sequences to the same length (returns tensor of shape [num_captions, max_len])\n",
    "padded_seqs = pad_sequence(all_seqs, batch_first=True, padding_value=word2idx['<pad>'])\n",
    "\n",
    "# Optionally get sequence lengths (not padded)\n",
    "seq_lengths = torch.tensor([len(seq) for seq in all_seqs], dtype=torch.long)\n",
    "\n",
    "# Preview\n",
    "print(f\"Padded shape: {padded_seqs.shape}\")\n",
    "print(f\"First padded caption: {padded_seqs[0]}\")\n",
    "print(f\"Original length: {seq_lengths[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5851840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab mappings saved.\n"
     ]
    }
   ],
   "source": [
    "save_dir = Path(\"../data/processed/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save word2idx\n",
    "with open(save_dir / \"word2idx.json\", 'w') as f:\n",
    "    json.dump(word2idx, f)\n",
    "\n",
    "# Save idx2word\n",
    "with open(save_dir / \"idx2word.json\", 'w') as f:\n",
    "    json.dump(idx2word, f)\n",
    "\n",
    "print(\"Vocab mappings saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc4f443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption tensors saved.\n"
     ]
    }
   ],
   "source": [
    "torch.save(padded_seqs, save_dir / \"padded_captions.pt\")\n",
    "torch.save(seq_lengths, save_dir / \"caption_lengths.pt\")\n",
    "\n",
    "print(\"Caption tensors saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96bb3ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image-caption sequences saved.\n"
     ]
    }
   ],
   "source": [
    "with open(save_dir / \"image_caption_seqs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(image_caption_seqs, f)\n",
    "\n",
    "print(\"Image-caption sequences saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1d3994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, image_folder, captions_tensor, lengths_tensor, image_filenames, transform=None):\n",
    "        \"\"\"\n",
    "        image_folder: path to folder with images (e.g., Flicker8k_Dataset)\n",
    "        captions_tensor: Tensor of padded caption sequences\n",
    "        lengths_tensor: Tensor of original caption lengths\n",
    "        image_filenames: list of image filenames (one per caption)\n",
    "        transform: torchvision transforms for the image\n",
    "        \"\"\"\n",
    "        self.image_folder = image_folder\n",
    "        self.captions = captions_tensor\n",
    "        self.lengths = lengths_tensor\n",
    "        self.image_filenames = image_filenames\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_folder, self.image_filenames[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Get caption and length\n",
    "        caption = self.captions[idx]\n",
    "        length = self.lengths[idx]\n",
    "\n",
    "        return image, caption, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "698cf8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")  # If utils is in the parent folder\n",
    "# or\n",
    "sys.path.append(\"../\")  # Adjust based on where your notebook is relative to the repo root\n",
    "from utils.dataloader import get_transforms, load_split_ids\n",
    "from utils.caption_dataset import CaptionDataset\n",
    "\n",
    "train_ids = load_split_ids(\"../data/Flickr8k_text/Flickr_8k.trainImages.txt\")\n",
    "val_ids = train_ids[-1000:]        # Last 1000 as validation (or use dev split)\n",
    "train_ids = train_ids[:-1000]\n",
    "\n",
    "transform = get_transforms('train')\n",
    "\n",
    "# Map image IDs to their corresponding caption tensors\n",
    "# Flatten caption sequences and match with filenames\n",
    "image_filenames = []\n",
    "caption_tensors = []\n",
    "lengths = []\n",
    "\n",
    "for img_id, captions in image_caption_seqs.items():\n",
    "    if img_id in train_ids:\n",
    "        for seq in captions:\n",
    "            image_filenames.append(img_id)\n",
    "            caption_tensors.append(torch.tensor(seq))\n",
    "            lengths.append(len(seq))\n",
    "\n",
    "# Pad\n",
    "padded_seqs = pad_sequence(caption_tensors, batch_first=True, padding_value=word2idx['<pad>'])\n",
    "lengths_tensor = torch.tensor(lengths)\n",
    "\n",
    "# Build dataset\n",
    "train_dataset = CaptionDataset(\n",
    "    image_folder=\"../data/Flicker8k_Dataset\",\n",
    "    captions_tensor=padded_seqs,\n",
    "    lengths_tensor=lengths_tensor,\n",
    "    image_filenames=image_filenames,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ba2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
