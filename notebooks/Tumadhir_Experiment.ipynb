{"cells":[{"cell_type":"markdown","source":["![image.png](data:image/png;base64,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)\n","\n","\n","**Assessment 3: Image Captioning Project**\n","\n","**94691 – Deep Learning Autumn 2025**\n","\n","Experiment - Tumadhir Alsharhan - 25605852"],"metadata":{"id":"THC4PVMDQbLe"},"id":"THC4PVMDQbLe"},{"cell_type":"code","source":["# #STEP 1: UNINSTALL any conflicting versions\n","# !pip uninstall -y numpy torch torchvision torchaudio\n","\n","# # STEP 2: INSTALL known stable versions (Colab-compatible)\n","# !pip install numpy==1.26.4 torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n","\n","# # STEP 3: Reinstall your external dependency\n","# !pip install efficientnet_pytorch"],"metadata":{"id":"ldZDFFH3Cfec"},"id":"ldZDFFH3Cfec","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Clean install missing dependencies\n","# !pip install numpy==1.26.4 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu118\n","\n","# # Optional: Reinstall EfficientNet (just in case)\n","# !pip install efficientnet_pytorch"],"metadata":{"id":"Lu8mLH5OIzeJ"},"id":"Lu8mLH5OIzeJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Uninstall conflicting numpy version if exists\n","# !pip uninstall -y numpy\n","\n","# # Install the last stable numpy 1.x version compatible with PyTorch\n","# !pip install numpy==1.26.3\n","\n","# # Install torchvision version compatible with PyTorch 2.6.0+cu124\n","# !pip install torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu124\n","\n","# # Optional (if needed): Reinstall EfficientNet\n","# !pip install efficientnet_pytorch --quiet"],"metadata":{"id":"b9xvi3dPKkng"},"id":"b9xvi3dPKkng","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Google Colab Settings"],"metadata":{"id":"IHh7m-RrPXoQ"},"id":"IHh7m-RrPXoQ"},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RaBM6wFCPV3w","outputId":"4b7953ba-a26b-423e-b12b-501f4c675617"},"id":"RaBM6wFCPV3w","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Change working directory\n","import os\n","project_path = '/content/drive/MyDrive/Colab Notebooks/DL-Assignment 3'\n","os.chdir(project_path)"],"metadata":{"id":"ziNIc_4XP7AO"},"id":"ziNIc_4XP7AO","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s_HLi1SBRhlF","outputId":"7e1bc784-a2e9-45e2-83a5-6cfc903125bc"},"id":"s_HLi1SBRhlF","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mdata\u001b[0m/  \u001b[01;34mnotebooks\u001b[0m/  README.md  requirements.txt  \u001b[01;34mutils\u001b[0m/\n"]}]},{"cell_type":"markdown","id":"d812846f","metadata":{"id":"d812846f"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"id":"bcc0c770","metadata":{"id":"bcc0c770","collapsed":true},"outputs":[],"source":["# #import libraries\n","\n","# import json\n","# import pickle\n","# from pathlib import Path\n","# import pandas as pd\n","# import numpy as np\n","# import torch\n","# import torch.nn as nn\n","# from torch.nn.utils.rnn import pad_sequence\n","# from torch.utils.data import DataLoader\n","# import torchvision.models as models\n","# from nltk.translate.bleu_score import corpus_bleu\n","# from tqdm import tqdm\n","# !pip install efficientnet_pytorch"]},{"cell_type":"code","source":["#Add utils to path to import project modules\n","import sys\n","sys.path.append('utils')"],"metadata":{"id":"YHk84EShRsjZ"},"id":"YHk84EShRsjZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from utils.dataloader import get_transforms, load_split_ids, build_caption_dataset\n","from utils.caption_dataset import CaptionDataset"],"metadata":{"id":"6BPCTTCCR4vh"},"id":"6BPCTTCCR4vh","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Confirm if the class was imported\n","print(CaptionDataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gwaOqRB3Zp3v","outputId":"e7400fd8-1f1c-486a-830a-331489490df5"},"id":"gwaOqRB3Zp3v","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'utils.caption_dataset.CaptionDataset'>\n"]}]},{"cell_type":"markdown","source":["## Load Datasets"],"metadata":{"id":"UZr6ML-kSM5p"},"id":"UZr6ML-kSM5p"},{"cell_type":"code","source":["# Load and clean vocabulary and captions\n","import os\n","import json\n","import pickle\n","\n","# Load vocab and caption data\n","processed_path = os.path.join(project_path, 'data', 'processed')\n","with open(os.path.join(processed_path, 'word2idx.json'), 'r') as f:\n","    word2idx = json.load(f)\n","\n","with open(os.path.join(processed_path, 'image_caption_seqs.pkl'), 'rb') as f:\n","    image_caption_seqs = pickle.load(f)\n","\n","# Step 1: Compute vocab_size safely based on actual data\n","flat_indices = [idx for seqs in image_caption_seqs.values() for seq in seqs for idx in seq]\n","true_max_index = max(flat_indices)\n","vocab_size = true_max_index + 1\n","\n","print(\"True max caption index:\", true_max_index)\n","print(\"Updated vocab_size:\", vocab_size)\n","\n","# Step 2: Clean invalid indices >= vocab_size and remove empty sequences\n","cleaned_caption_seqs = {}\n","for img_id, seqs in image_caption_seqs.items():\n","    valid_seqs = []\n","    for seq in seqs:\n","        cleaned = [i for i in seq if i < vocab_size]\n","        if len(cleaned) > 1:  # Ensure useful caption\n","            valid_seqs.append(cleaned)\n","    if valid_seqs:\n","        cleaned_caption_seqs[img_id] = valid_seqs\n","\n","# Overwrite with cleaned data\n","image_caption_seqs = cleaned_caption_seqs\n","print(\"Cleaned image-caption pairs:\", len(image_caption_seqs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9lB0Bh9eV1_h","outputId":"086e7b43-56e4-4b2a-8a03-073e2cdc03d6"},"id":"9lB0Bh9eV1_h","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True max caption index: 2989\n","Updated vocab_size: 2990\n","Cleaned image-caption pairs: 8092\n"]}]},{"cell_type":"code","execution_count":null,"id":"c4b33923","metadata":{"id":"c4b33923"},"outputs":[],"source":["# Load splits\n","train_ids = load_split_ids(os.path.join(project_path, 'data', 'Flickr8k_text', 'Flickr_8k.trainImages.txt'))\n","val_ids   = load_split_ids(os.path.join(project_path, 'data', 'Flickr8k_text', 'Flickr_8k.devImages.txt'))\n","test_ids  = load_split_ids(os.path.join(project_path, 'data', 'Flickr8k_text', 'Flickr_8k.testImages.txt'))\n","\n","# Build datasets *after* cleaning and correct vocab size\n","train_dataset = build_caption_dataset(train_ids, image_caption_seqs, word2idx, image_folder, transform_train)\n","val_dataset   = build_caption_dataset(val_ids, image_caption_seqs, word2idx, image_folder, transform_val)\n","test_dataset  = build_caption_dataset(test_ids, image_caption_seqs, word2idx, image_folder, transform_val)"]},{"cell_type":"markdown","source":["## Data Preprocesing (Just Images)"],"metadata":{"id":"-xqexvqYVvWX"},"id":"-xqexvqYVvWX"},{"cell_type":"code","execution_count":null,"id":"65931e1a","metadata":{"id":"65931e1a"},"outputs":[],"source":["# Set image folder path\n","image_folder = os.path.join(project_path, 'data', 'Flickr8k_Dataset', 'Flicker8k_Dataset')\n","\n","# Define transforms\n","transform_train = get_transforms(\"train\")\n","transform_val   = get_transforms(\"val\")\n","\n","# Build datasets using shared util function\n","train_dataset = build_caption_dataset(train_ids, image_caption_seqs, word2idx, image_folder, transform_train)\n","val_dataset   = build_caption_dataset(val_ids, image_caption_seqs, word2idx, image_folder, transform_val)\n","test_dataset  = build_caption_dataset(test_ids, image_caption_seqs, word2idx, image_folder, transform_val)"]},{"cell_type":"markdown","source":["## Experiment Architecture"],"metadata":{"id":"NVXra222Z-We"},"id":"NVXra222Z-We"},{"cell_type":"code","source":["# EfficientNet-B3 Encoder\n","from efficientnet_pytorch import EfficientNet\n","\n","class EncoderEfficientNetB3(nn.Module):\n","    def __init__(self, encoded_image_size=14):\n","        super().__init__()\n","        self.enc_image_size = encoded_image_size\n","        self.backbone = EfficientNet.from_pretrained('efficientnet-b3')\n","\n","        # Remove the classifier\n","        self.backbone._fc = nn.Identity()\n","\n","        # Use only the convolutional backbone\n","        self.features = self.backbone.extract_features\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n","\n","        for param in self.backbone.parameters():\n","            param.requires_grad = False\n","\n","    def forward(self, images):\n","        x = self.features(images)                # (batch_size, 1536, H, W)\n","        x = self.adaptive_pool(x)                # (batch_size, 1536, 14, 14)\n","        x = x.permute(0, 2, 3, 1)                # (batch_size, 14, 14, 1536)\n","        return x"],"metadata":{"id":"Bl6p1gpxikBI"},"id":"Bl6p1gpxikBI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Bahdanau (Additive) Attention\n","class BahdanauAttention(nn.Module):\n","    def __init__(self, encoder_dim, hidden_dim, attention_dim):\n","        super().__init__()\n","        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n","        self.decoder_att = nn.Linear(hidden_dim, attention_dim)\n","        self.full_att = nn.Linear(attention_dim, 1)\n","\n","    def forward(self, encoder_out, decoder_hidden):\n","        att1 = self.encoder_att(encoder_out)              # (batch, num_pixels, attention_dim)\n","        att2 = self.decoder_att(decoder_hidden).unsqueeze(1)  # (batch, 1, attention_dim)\n","        att = torch.tanh(att1 + att2)\n","        energy = self.full_att(att).squeeze(2)            # (batch, num_pixels)\n","        alpha = torch.softmax(energy, dim=1)              # (batch, num_pixels)\n","        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch, encoder_dim)\n","        return context, alpha"],"metadata":{"id":"JW4dRgYbjQQY"},"id":"JW4dRgYbjQQY","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Decoder with GRU and Bahdanau Attention\n","class DecoderGRUWithBahdanau(nn.Module):\n","    def __init__(self, attention, embed_dim, hidden_dim, vocab_size, encoder_dim=1536, dropout=0.5):\n","        super().__init__()\n","        self.attention = attention\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.gru = nn.GRU(embed_dim + encoder_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","        self.init_h = nn.Linear(encoder_dim, hidden_dim)\n","\n","    def forward(self, encoder_out, captions):\n","        batch_size = encoder_out.size(0)\n","        encoder_out = encoder_out.view(batch_size, -1, encoder_out.size(-1))  # (batch, num_pixels, encoder_dim)\n","\n","        # Added safety check\n","        if torch.any(captions >= self.embedding.num_embeddings):\n","            raise ValueError(\"Found index in captions >= vocab_size. Check data preprocessing.\")\n","\n","        embeddings = self.embedding(captions)  # (batch, seq_len, embed_dim)\n","        h = self.init_h(encoder_out.mean(dim=1)).unsqueeze(0)  # (1, batch, hidden_dim)\n","\n","        outputs = torch.zeros(batch_size, captions.size(1), self.fc.out_features).to(captions.device)\n","\n","        for t in range(captions.size(1)):\n","            context, _ = self.attention(encoder_out, h[-1])\n","            input_t = torch.cat([embeddings[:, t, :], context], dim=1).unsqueeze(1)\n","            out, h = self.gru(input_t, h)\n","            outputs[:, t, :] = self.fc(self.dropout(out.squeeze(1)))\n","\n","        return outputs"],"metadata":{"id":"V60Uvm4ZjgkR"},"id":"V60Uvm4ZjgkR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function for training the model\n","\n","def train_model(model, train_dataset, val_dataset, word2idx, device='cuda',\n","                batch_size=32, epochs=20, patience=3, lr=1e-4):\n","\n","    pad_idx = word2idx['<pad>']\n","    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4, pin_memory=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n","\n","    best_val_loss = float('inf')\n","    patience_counter = 0\n","\n","    # Track metrics per epoch\n","    metrics_log = []\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        train_losses = []\n","\n","        tqdm_train = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\")\n","        for images, captions, _ in tqdm_train:\n","            images, captions = images.to(device), captions.to(device)\n","\n","            # Debug: catch OOV tokens\n","            if torch.any(captions >= model.decoder.embedding.num_embeddings):\n","                offending = captions[captions >= model.decoder.embedding.num_embeddings].tolist()\n","                print(\"Invalid caption indices found:\", offending)\n","                print(\"Caption max index:\", captions.max().item(),\n","                      \"| Embedding size:\", model.decoder.embedding.num_embeddings)\n","                raise ValueError(\"Aborting training: Found index in captions >= vocab_size\")\n","\n","            optimizer.zero_grad()\n","            outputs = model(images, captions[:, :-1])\n","            loss = criterion(outputs.reshape(-1, outputs.size(-1)), captions[:, 1:].reshape(-1))\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_losses.append(loss.item())\n","            tqdm_train.set_postfix(loss=np.mean(train_losses))\n","\n","        avg_train_loss = np.mean(train_losses)\n","\n","        # Validation\n","        model.eval()\n","        val_losses = []\n","        references = []\n","        hypotheses = []\n","\n","        tqdm_val = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Validation]\")\n","        with torch.no_grad():\n","            for images, captions, lengths in tqdm_val:\n","                images, captions = images.to(device), captions.to(device)\n","                outputs = model(images, captions[:, :-1])\n","                loss = criterion(outputs.reshape(-1, outputs.size(-1)), captions[:, 1:].reshape(-1))\n","                val_losses.append(loss.item())\n","\n","                # BLEU prep\n","                preds = torch.argmax(outputs, dim=2)\n","                for ref, pred in zip(captions, preds):\n","                    ref_tokens = [w for w in ref.tolist() if w not in {pad_idx, word2idx['<start>'], word2idx['<end>']}]\n","                    pred_tokens = [w for w in pred.tolist() if w not in {pad_idx, word2idx['<start>'], word2idx['<end>']}]\n","                    references.append([ref_tokens])\n","                    hypotheses.append(pred_tokens)\n","\n","        avg_val_loss = np.mean(val_losses)\n","        scheduler.step(avg_val_loss)\n","\n","        # BLEU metrics\n","        bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n","        bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n","        bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n","        bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n","\n","        # Print results\n","        print(f\"\\nEpoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n","        print(f\"BLEU-1 = {bleu1:.4f}, BLEU-2 = {bleu2:.4f}, BLEU-3 = {bleu3:.4f}, BLEU-4 = {bleu4:.4f}\")\n","\n","        # Save metrics\n","        metrics_log.append({\n","            'epoch': epoch + 1,\n","            'train_loss': avg_train_loss,\n","            'val_loss': avg_val_loss,\n","            'bleu1': bleu1,\n","            'bleu2': bleu2,\n","            'bleu3': bleu3,\n","            'bleu4': bleu4,\n","        })\n","\n","        # Early stopping\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            patience_counter = 0\n","            torch.save(model.state_dict(), \"best_model.pt\")\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(f\"Early stopping triggered at epoch {epoch+1}\")\n","                break\n","\n","    print(\"Training complete.\")\n","    return model, metrics_log"],"metadata":{"id":"gTqUdaStjjem"},"id":"gTqUdaStjjem","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"QEUrPzmakVoZ"},"id":"QEUrPzmakVoZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CaptioningModel(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(CaptioningModel, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, images, captions):\n","        encoder_out = self.encoder(images)\n","        outputs = self.decoder(encoder_out, captions)\n","        return outputs"],"metadata":{"id":"Zl4oM7IpmGeD"},"id":"Zl4oM7IpmGeD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import glob\n","image_folder = os.path.join(project_path, 'data', 'Flickr8k_Dataset', 'Flicker8k_Dataset')\n","jpg_files = glob.glob(os.path.join(image_folder, '*.jpg'))\n","print(f\"Found {len(jpg_files)} image files.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xoex9ZH7GQNk","outputId":"e5862ff0-75ea-45ea-a3aa-06fa20962818"},"id":"xoex9ZH7GQNk","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 8091 image files.\n"]}]},{"cell_type":"code","source":["# Set hyperparameters\n","embed_dim = 256\n","hidden_dim = 512\n","attention_dim = 256\n","dropout = 0.5\n","# vocab_size = len(word2idx)\n","# Use the cleaned vocab_size computed earlier\n","\n","# Instantiate components\n","encoder = EncoderEfficientNetB3(encoded_image_size=14)\n","attention = BahdanauAttention(encoder_dim=1536, hidden_dim=hidden_dim, attention_dim=attention_dim)\n","decoder = DecoderGRUWithBahdanau(\n","    attention=attention,\n","    embed_dim=embed_dim,\n","    hidden_dim=hidden_dim,\n","    vocab_size=vocab_size,\n","    encoder_dim=1536,\n","    dropout=dropout\n",")\n","\n","# Combine encoder and decoder\n","model = CaptioningModel(encoder, decoder).to(device)\n","\n","\n","# Start training the model\n","model, log = train_model(\n","    model=model,\n","    train_dataset=train_dataset,\n","    val_dataset=val_dataset,\n","    word2idx=word2idx,\n","    device=device,\n","    batch_size=8,\n","    epochs=20,\n","    patience=3,\n","    lr=1e-4\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CBUPIgBC-4YQ","outputId":"2484660b-accb-4861-ab04-454a64d347bb"},"id":"CBUPIgBC-4YQ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded pretrained weights for efficientnet-b3\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1 [Training]: 100%|██████████| 3750/3750 [12:34<00:00,  4.97it/s, loss=4.11]\n","Epoch 1 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  8.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: Train Loss = 4.1061, Val Loss = 3.6084\n","BLEU-1 = 0.3262, BLEU-2 = 0.1657, BLEU-3 = 0.0864, BLEU-4 = 0.0465\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2 [Training]: 100%|██████████| 3750/3750 [11:36<00:00,  5.38it/s, loss=3.43]\n","Epoch 2 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  8.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 2: Train Loss = 3.4252, Val Loss = 3.3311\n","BLEU-1 = 0.3544, BLEU-2 = 0.1890, BLEU-3 = 0.1039, BLEU-4 = 0.0570\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3 [Training]: 100%|██████████| 3750/3750 [11:39<00:00,  5.36it/s, loss=3.19]\n","Epoch 3 [Validation]: 100%|██████████| 625/625 [01:10<00:00,  8.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 3: Train Loss = 3.1914, Val Loss = 3.1941\n","BLEU-1 = 0.3670, BLEU-2 = 0.1979, BLEU-3 = 0.1097, BLEU-4 = 0.0607\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4 [Training]: 100%|██████████| 3750/3750 [11:39<00:00,  5.36it/s, loss=3.04]\n","Epoch 4 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  8.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 4: Train Loss = 3.0436, Val Loss = 3.1109\n","BLEU-1 = 0.3777, BLEU-2 = 0.2085, BLEU-3 = 0.1199, BLEU-4 = 0.0677\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5 [Training]: 100%|██████████| 3750/3750 [11:38<00:00,  5.37it/s, loss=2.94]\n","Epoch 5 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  8.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 5: Train Loss = 2.9374, Val Loss = 3.0522\n","BLEU-1 = 0.3816, BLEU-2 = 0.2116, BLEU-3 = 0.1218, BLEU-4 = 0.0691\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6 [Training]: 100%|██████████| 3750/3750 [11:38<00:00,  5.37it/s, loss=2.85]\n","Epoch 6 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  9.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 6: Train Loss = 2.8494, Val Loss = 3.0086\n","BLEU-1 = 0.3883, BLEU-2 = 0.2168, BLEU-3 = 0.1258, BLEU-4 = 0.0718\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7 [Training]: 100%|██████████| 3750/3750 [11:38<00:00,  5.37it/s, loss=2.78]\n","Epoch 7 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  8.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 7: Train Loss = 2.7812, Val Loss = 2.9798\n","BLEU-1 = 0.3916, BLEU-2 = 0.2187, BLEU-3 = 0.1274, BLEU-4 = 0.0736\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8 [Training]: 100%|██████████| 3750/3750 [11:37<00:00,  5.37it/s, loss=2.72]\n","Epoch 8 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  8.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 8: Train Loss = 2.7176, Val Loss = 2.9554\n","BLEU-1 = 0.3948, BLEU-2 = 0.2220, BLEU-3 = 0.1306, BLEU-4 = 0.0754\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9 [Training]: 100%|██████████| 3750/3750 [11:38<00:00,  5.37it/s, loss=2.66]\n","Epoch 9 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  9.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 9: Train Loss = 2.6647, Val Loss = 2.9400\n","BLEU-1 = 0.3915, BLEU-2 = 0.2187, BLEU-3 = 0.1270, BLEU-4 = 0.0721\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10 [Training]: 100%|██████████| 3750/3750 [11:38<00:00,  5.37it/s, loss=2.61]\n","Epoch 10 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  8.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 10: Train Loss = 2.6122, Val Loss = 2.9190\n","BLEU-1 = 0.3968, BLEU-2 = 0.2239, BLEU-3 = 0.1318, BLEU-4 = 0.0760\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11 [Training]: 100%|██████████| 3750/3750 [11:37<00:00,  5.38it/s, loss=2.57]\n","Epoch 11 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  8.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 11: Train Loss = 2.5713, Val Loss = 2.9124\n","BLEU-1 = 0.3993, BLEU-2 = 0.2261, BLEU-3 = 0.1336, BLEU-4 = 0.0774\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12 [Training]: 100%|██████████| 3750/3750 [11:38<00:00,  5.37it/s, loss=2.53]\n","Epoch 12 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  9.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 12: Train Loss = 2.5289, Val Loss = 2.9045\n","BLEU-1 = 0.3989, BLEU-2 = 0.2250, BLEU-3 = 0.1318, BLEU-4 = 0.0762\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13 [Training]: 100%|██████████| 3750/3750 [11:40<00:00,  5.36it/s, loss=2.49]\n","Epoch 13 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  8.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 13: Train Loss = 2.4915, Val Loss = 2.8953\n","BLEU-1 = 0.4023, BLEU-2 = 0.2279, BLEU-3 = 0.1348, BLEU-4 = 0.0778\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14 [Training]: 100%|██████████| 3750/3750 [11:38<00:00,  5.37it/s, loss=2.45]\n","Epoch 14 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  9.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 14: Train Loss = 2.4542, Val Loss = 2.8863\n","BLEU-1 = 0.4049, BLEU-2 = 0.2296, BLEU-3 = 0.1359, BLEU-4 = 0.0785\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15 [Training]: 100%|██████████| 3750/3750 [11:38<00:00,  5.37it/s, loss=2.42]\n","Epoch 15 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  8.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 15: Train Loss = 2.4207, Val Loss = 2.8808\n","BLEU-1 = 0.4048, BLEU-2 = 0.2288, BLEU-3 = 0.1354, BLEU-4 = 0.0783\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16 [Training]: 100%|██████████| 3750/3750 [11:38<00:00,  5.37it/s, loss=2.39]\n","Epoch 16 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  9.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 16: Train Loss = 2.3911, Val Loss = 2.8834\n","BLEU-1 = 0.4019, BLEU-2 = 0.2271, BLEU-3 = 0.1336, BLEU-4 = 0.0763\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 17 [Training]: 100%|██████████| 3750/3750 [11:37<00:00,  5.38it/s, loss=2.36]\n","Epoch 17 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  9.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 17: Train Loss = 2.3592, Val Loss = 2.8777\n","BLEU-1 = 0.4067, BLEU-2 = 0.2294, BLEU-3 = 0.1353, BLEU-4 = 0.0778\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 18 [Training]: 100%|██████████| 3750/3750 [11:37<00:00,  5.37it/s, loss=2.33]\n","Epoch 18 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  8.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 18: Train Loss = 2.3302, Val Loss = 2.8797\n","BLEU-1 = 0.4051, BLEU-2 = 0.2295, BLEU-3 = 0.1366, BLEU-4 = 0.0793\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 19 [Training]: 100%|██████████| 3750/3750 [11:37<00:00,  5.38it/s, loss=2.3]\n","Epoch 19 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  8.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 19: Train Loss = 2.3005, Val Loss = 2.8796\n","BLEU-1 = 0.4035, BLEU-2 = 0.2299, BLEU-3 = 0.1366, BLEU-4 = 0.0789\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 20 [Training]: 100%|██████████| 3750/3750 [11:37<00:00,  5.37it/s, loss=2.28]\n","Epoch 20 [Validation]: 100%|██████████| 625/625 [01:09<00:00,  8.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 20: Train Loss = 2.2761, Val Loss = 2.8790\n","BLEU-1 = 0.4076, BLEU-2 = 0.2316, BLEU-3 = 0.1378, BLEU-4 = 0.0794\n","Early stopping triggered at epoch 20\n","Training complete.\n"]}]},{"cell_type":"code","source":["# Save trained model weights and full model\n","save_path = os.path.join(project_path, 'data')\n","os.makedirs(save_path, exist_ok=True)\n","\n","torch.save(model.state_dict(), os.path.join(save_path, 'experiment_Tumadhir_model2.pth'))\n","torch.save(model, os.path.join(save_path, 'experiment_Tumadhir_model2_full.pth'))"],"metadata":{"id":"4fiZsatMlztr"},"id":"4fiZsatMlztr","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import pandas as pd\n","# df_log = pd.DataFrame(log)\n","# df_log.to_csv(os.path.join(save_path, 'experiment_Tumadhir_metrics.csv'), index=False)\n"],"metadata":{"id":"KX_5mIAToKCh"},"id":"KX_5mIAToKCh","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}