{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports and Setup**"
      ],
      "metadata": {
        "id": "s_M-qtCGX1r9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "tTcWqX0xX0tA",
        "outputId": "29b3ab06-d763-4f44-a5c0-9c12a4adf8ce"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4e67eb9348e8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_transforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_split_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_caption_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaption_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCaptionDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")  # Adjust if needed based on directory\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from utils.dataloader import get_transforms, load_split_ids, build_caption_dataset\n",
        "from utils.caption_dataset import CaptionDataset\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Vocabulary and Caption Data**"
      ],
      "metadata": {
        "id": "jyjWJN_jX_0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load vocab\n",
        "with open(\"../data/processed/word2idx.json\", \"r\") as f:\n",
        "    word2idx = json.load(f)\n",
        "\n",
        "with open(\"../data/processed/image_caption_seqs.pkl\", \"rb\") as f:\n",
        "    image_caption_seqs = pickle.load(f)\n",
        "\n",
        "vocab_size = len(word2idx)"
      ],
      "metadata": {
        "id": "HuSCYN0bX_oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Preparation**"
      ],
      "metadata": {
        "id": "N6N9NBI4YEt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image splits\n",
        "train_ids = load_split_ids(\"../data/Flickr8k_text/Flickr_8k.trainImages.txt\")\n",
        "val_ids   = load_split_ids(\"../data/Flickr8k_text/Flickr_8k.devImages.txt\")\n",
        "test_ids  = load_split_ids(\"../data/Flickr8k_text/Flickr_8k.testImages.txt\")\n",
        "\n",
        "# Transforms and image folder path\n",
        "transform_train = get_transforms(\"train\")\n",
        "transform_val = get_transforms(\"val\")\n",
        "image_folder = \"../data/Flicker8k_Dataset\"\n",
        "\n",
        "# Build datasets\n",
        "train_dataset = build_caption_dataset(train_ids, image_caption_seqs, word2idx, image_folder, transform_train)\n",
        "val_dataset   = build_caption_dataset(val_ids, image_caption_seqs, word2idx, image_folder, transform_val)"
      ],
      "metadata": {
        "id": "-wHR16ptYElz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoder with EfficientNet-B3**"
      ],
      "metadata": {
        "id": "1zmjTpK-YKlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "class EncoderEfficientNet(nn.Module):\n",
        "    def __init__(self, encoded_image_size=14, fine_tune=True):\n",
        "        super(EncoderEfficientNet, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        # Load pretrained EfficientNet-B3\n",
        "        self.efficientnet = EfficientNet.from_pretrained('efficientnet-b3')\n",
        "\n",
        "        # Remove the final classification layer\n",
        "        self.features = self.efficientnet.extract_features\n",
        "\n",
        "        # Resize feature map to fixed size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "\n",
        "        self.fine_tune(fine_tune)\n",
        "\n",
        "    def forward(self, images):\n",
        "        out = self.features(images)                      # (batch_size, channels, H, W)\n",
        "        out = self.adaptive_pool(out)                    # (batch_size, channels, 14, 14)\n",
        "        out = out.permute(0, 2, 3, 1)                     # (batch_size, 14, 14, channels)\n",
        "        return out\n",
        "\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "        for p in self.efficientnet.parameters():\n",
        "            p.requires_grad = False\n",
        "        if fine_tune:\n",
        "            for name, param in self.efficientnet.named_parameters():\n",
        "                if \"blocks.5\" in name or \"blocks.6\" in name or \"blocks.7\" in name:\n",
        "                    param.requires_grad = True"
      ],
      "metadata": {
        "id": "6og2X1n-YKb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bahdanau Attention**"
      ],
      "metadata": {
        "id": "uX10kdMoYP4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, encoder_dim, hidden_dim, attention_dim):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
        "        self.decoder_att = nn.Linear(hidden_dim, attention_dim)\n",
        "        self.full_att = nn.Linear(attention_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, encoder_out, hidden_state):\n",
        "        # encoder_out: (batch_size, num_pixels, encoder_dim)\n",
        "        # hidden_state: (batch_size, hidden_dim)\n",
        "\n",
        "        att1 = self.encoder_att(encoder_out)            # (batch_size, num_pixels, attention_dim)\n",
        "        att2 = self.decoder_att(hidden_state).unsqueeze(1)  # (batch_size, 1, attention_dim)\n",
        "        att = self.full_att(self.relu(att1 + att2)).squeeze(2)  # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)                       # (batch_size, num_pixels)\n",
        "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
        "        return context, alpha"
      ],
      "metadata": {
        "id": "FrSQELXpYPuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRU Decoder with Attention**"
      ],
      "metadata": {
        "id": "WTZZD1oCYa_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderGRUWithAttention(nn.Module):\n",
        "    def __init__(self, attention_dim, embed_dim, hidden_dim, vocab_size, encoder_dim=1536, dropout=0.5):\n",
        "        super(DecoderGRUWithAttention, self).__init__()\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = BahdanauAttention(encoder_dim, hidden_dim, attention_dim)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "        self.gru = nn.GRU(embed_dim + encoder_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.init_hidden = nn.Linear(encoder_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, encoder_out, captions):\n",
        "        batch_size = captions.size(0)\n",
        "        max_len = captions.size(1)\n",
        "\n",
        "        encoder_out = encoder_out.view(batch_size, -1, self.encoder_dim)  # (B, num_pixels, encoder_dim)\n",
        "        embeddings = self.embedding(captions)                             # (B, max_len, embed_dim)\n",
        "        hidden = self.init_hidden(encoder_out.mean(dim=1)).unsqueeze(0)  # (1, B, hidden_dim)\n",
        "\n",
        "        outputs = torch.zeros(batch_size, max_len, self.vocab_size).to(captions.device)\n",
        "\n",
        "        for t in range(max_len):\n",
        "            context, _ = self.attention(encoder_out, hidden.squeeze(0))  # context: (B, encoder_dim)\n",
        "            input_t = torch.cat([embeddings[:, t, :], context], dim=1).unsqueeze(1)  # (B, 1, embed+encoder)\n",
        "            output, hidden = self.gru(input_t, hidden)  # output: (B, 1, hidden_dim)\n",
        "            output = self.fc(self.dropout_layer(output.squeeze(1)))  # (B, vocab_size)\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "l6BEN9RoYa3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Wrapper**"
      ],
      "metadata": {
        "id": "4lxzWhQIYgLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lass CaptioningModel(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(CaptioningModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        encoder_out = self.encoder(images)\n",
        "        outputs = self.decoder(encoder_out, captions)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "QTbBZe85YgCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Function**"
      ],
      "metadata": {
        "id": "2WWewrmEYkP3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yYMRgDh5YkER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instantiate and Train**"
      ],
      "metadata": {
        "id": "K8qEKUmcYslq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embed_dim = 256\n",
        "hidden_dim = 512\n",
        "attention_dim = 256\n",
        "dropout = 0.5\n",
        "\n",
        "encoder = EncoderEfficientNet(encoded_image_size=14, fine_tune=True)\n",
        "decoder = DecoderGRUWithAttention(\n",
        "    attention_dim=attention_dim,\n",
        "    embed_dim=embed_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    vocab_size=vocab_size,\n",
        "    encoder_dim=1536,  # EfficientNet-B3 output channels\n",
        "    dropout=dropout\n",
        ")\n",
        "\n",
        "model = CaptioningModel(encoder, decoder).to(device)\n",
        "\n",
        "# Train\n",
        "trained_model = train_model(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    word2idx=word2idx,\n",
        "    device=device,\n",
        "    batch_size=8,\n",
        "    epochs=20,\n",
        "    patience=3,\n",
        "    lr=1e-4\n",
        ")"
      ],
      "metadata": {
        "id": "xcusUM04YsSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save the Model**"
      ],
      "metadata": {
        "id": "Xvq54XS6YxcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"../data/experiment_tumadhir_model.pth\")\n",
        "torch.save(model, \"../data/experiment_tumadhir_model_full.pth\")"
      ],
      "metadata": {
        "id": "JoPPn9ElYxSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BLEU Score Evaluation on the Test Set**"
      ],
      "metadata": {
        "id": "sY7nI2bhZORn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model(model, test_dataset, word2idx, device='cuda', batch_size=32):\n",
        "    model.eval()\n",
        "    pad_idx = word2idx['<pad>']\n",
        "    start_idx = word2idx['<start>']\n",
        "    end_idx = word2idx['<end>']\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, captions, lengths in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n",
        "            images, captions = images.to(device), captions.to(device)\n",
        "            outputs = model(images, captions[:, :-1])  # Teacher forcing for consistency\n",
        "            preds = torch.argmax(outputs, dim=2)\n",
        "\n",
        "            for ref, pred in zip(captions, preds):\n",
        "                ref_tokens = [w for w in ref.tolist() if w not in {pad_idx, start_idx, end_idx}]\n",
        "                pred_tokens = [w for w in pred.tolist() if w not in {pad_idx, start_idx, end_idx}]\n",
        "                references.append([ref_tokens])\n",
        "                hypotheses.append(pred_tokens)\n",
        "\n",
        "    bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
        "    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
        "    bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
        "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "    print(f\"\\nFinal BLEU Scores on Test Set:\")\n",
        "    print(f\"BLEU-1 = {bleu1:.4f}\")\n",
        "    print(f\"BLEU-2 = {bleu2:.4f}\")\n",
        "    print(f\"BLEU-3 = {bleu3:.4f}\")\n",
        "    print(f\"BLEU-4 = {bleu4:.4f}\")"
      ],
      "metadata": {
        "id": "TsNWMLsGZOGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run Evaluation**"
      ],
      "metadata": {
        "id": "ihueshshZThP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(trained_model, test_dataset, word2idx, device=device, batch_size=32)"
      ],
      "metadata": {
        "id": "-hKsqFLdZTWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Caption Generation for Sample Images**"
      ],
      "metadata": {
        "id": "vUdcUTIdZ5CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def generate_caption(model, image_path, word2idx, idx2word, transform, max_len=20):\n",
        "    model.eval()\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_out = model.encoder(image_tensor)\n",
        "        encoder_out = encoder_out.view(1, -1, model.decoder.encoder_dim)\n",
        "\n",
        "        hidden = model.decoder.init_hidden(encoder_out.mean(dim=1)).unsqueeze(0)\n",
        "        word_input = torch.tensor([[word2idx[\"<start>\"]]]).to(device)\n",
        "\n",
        "        caption = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            embedding = model.decoder.embedding(word_input).squeeze(1)\n",
        "            context, _ = model.decoder.attention(encoder_out, hidden.squeeze(0))\n",
        "            input_t = torch.cat([embedding, context], dim=1).unsqueeze(1)\n",
        "            output, hidden = model.decoder.gru(input_t, hidden)\n",
        "            output = model.decoder.fc(output.squeeze(1))\n",
        "\n",
        "            word_idx = output.argmax(dim=1).item()\n",
        "            if word_idx == word2idx[\"<end>\"]:\n",
        "                break\n",
        "\n",
        "            caption.append(idx2word.get(str(word_idx), \"<unk>\"))\n",
        "            word_input = torch.tensor([[word_idx]]).to(device)\n",
        "\n",
        "    return \" \".join(caption)"
      ],
      "metadata": {
        "id": "nr2vGXA7Z44g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the Function to Generate a Caption"
      ],
      "metadata": {
        "id": "bJU6kptZaOYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load idx2word mapping if not already loaded\n",
        "with open(\"../data/processed/idx2word.json\") as f:\n",
        "    idx2word = json.load(f)\n",
        "\n",
        "# Apply the same transform as validation\n",
        "transform = get_transforms(\"val\")\n",
        "\n",
        "# Choose a test image from the Flickr8k dataset\n",
        "sample_img = \"../data/Flicker8k_Dataset/1000268201_693b08cb0e.jpg\"\n",
        "\n",
        "# Generate and print the caption\n",
        "caption = generate_caption(trained_model, sample_img, word2idx, idx2word, transform)\n",
        "print(\"Generated Caption:\", caption)"
      ],
      "metadata": {
        "id": "jcySB02gZ92y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}