{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQpALDCjNUeN",
        "outputId": "b69dfa04-7849-4a6e-9d20-b2efe18a1b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ],
      "metadata": {
        "id": "58m0FbQRqtQk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z3v76yhrN9XD"
      },
      "outputs": [],
      "source": [
        "import sys, os, json, pickle\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE0E9WCucZoM",
        "outputId": "ab19c7df-d589-4d87-aa5d-0e79857fa921"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xOSHIVvTNilR"
      },
      "outputs": [],
      "source": [
        "# === PATHS ===\n",
        "base_path = \"/content/drive/MyDrive/UTS/SEM 4/DL/AT3/AT3-DL-image-captioning\"\n",
        "sys.path.append(os.path.join(base_path))\n",
        "image_folder = os.path.join(base_path, \"data/Flicker8k_Dataset\")\n",
        "text_folder = os.path.join(base_path, \"data/Flickr8k_text\")\n",
        "processed_folder = os.path.join(base_path, \"data/processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R_yHpHbwO7LS"
      },
      "outputs": [],
      "source": [
        "# === LOAD UTILS ===\n",
        "from utils.dataloader import get_transforms, load_split_ids, build_caption_dataset\n",
        "from utils.caption_dataset import CaptionDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WQWZuy2jW71k"
      },
      "outputs": [],
      "source": [
        "# === LOAD VOCAB & SEQUENCES ===\n",
        "with open(os.path.join(processed_folder, \"word2idx.json\"), \"r\") as f:\n",
        "    word2idx = json.load(f)\n",
        "\n",
        "with open(os.path.join(processed_folder, \"image_caption_seqs.pkl\"), \"rb\") as f:\n",
        "    image_caption_seqs = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vmwKUiAUQHeo"
      },
      "outputs": [],
      "source": [
        "# === LOAD SPLITS & TRANSFORMS ===\n",
        "train_ids = load_split_ids(os.path.join(text_folder, \"Flickr_8k.trainImages.txt\"))\n",
        "val_ids   = load_split_ids(os.path.join(text_folder, \"Flickr_8k.devImages.txt\"))\n",
        "test_ids  = load_split_ids(os.path.join(text_folder, \"Flickr_8k.testImages.txt\"))\n",
        "\n",
        "transform_train = get_transforms(\"train\")\n",
        "transform_val   = get_transforms(\"val\")\n",
        "\n",
        "train_dataset = build_caption_dataset(train_ids, image_caption_seqs, word2idx, image_folder, transform_train)\n",
        "val_dataset   = build_caption_dataset(val_ids, image_caption_seqs, word2idx, image_folder, transform_val)\n",
        "test_dataset  = build_caption_dataset(test_ids, image_caption_seqs, word2idx, image_folder, transform_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5qpZG9KeQKgl"
      },
      "outputs": [],
      "source": [
        "# === ENCODER: DENSENET121 ===\n",
        "from torchvision.models import densenet121\n",
        "\n",
        "class DenseNetEncoder(nn.Module):\n",
        "    def __init__(self, encoded_image_size=7, fine_tune=True):\n",
        "        super(DenseNetEncoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "        densenet = densenet121(pretrained=True)\n",
        "        self.features = densenet.features\n",
        "        self.pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "        self.fine_tune(fine_tune)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.features(images)\n",
        "        out = self.pool(features)\n",
        "        out = out.permute(0, 2, 3, 1)\n",
        "        return out\n",
        "\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "        for p in self.features.parameters():\n",
        "            p.requires_grad = False\n",
        "        if fine_tune:\n",
        "            for c in list(self.features.children())[-4:]:\n",
        "                for p in c.parameters():\n",
        "                    p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "12EEX5HMQe87"
      },
      "outputs": [],
      "source": [
        "# === ATTENTION ===\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim, hidden_dim, attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
        "        self.decoder_att = nn.Linear(hidden_dim, attention_dim)\n",
        "        self.full_att = nn.Linear(attention_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        att1 = self.encoder_att(encoder_out)\n",
        "        att2 = self.decoder_att(decoder_hidden).unsqueeze(1)\n",
        "        att = self.full_att(self.relu(att1 + att2)).squeeze(2)\n",
        "        alpha = self.softmax(att)\n",
        "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
        "        return context, alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QLxKCuNGQjia"
      },
      "outputs": [],
      "source": [
        "# === DECODER ===\n",
        "class DecoderRNNWithAttention(nn.Module):\n",
        "    def __init__(self, attention_dim, embed_dim, hidden_dim, vocab_size, encoder_dim=1024, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "        self.attention = Attention(encoder_dim, hidden_dim, attention_dim)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.dropout_layer = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, hidden_dim, bias=True)\n",
        "        self.init_h = nn.Linear(encoder_dim, hidden_dim)\n",
        "        self.init_c = nn.Linear(encoder_dim, hidden_dim)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, encoder_out, captions):\n",
        "        batch_size = captions.size(0)\n",
        "        max_len = captions.size(1)\n",
        "        encoder_out = encoder_out.view(batch_size, -1, self.encoder_dim)\n",
        "\n",
        "        # === DEBUG: Check for bad token indices ===\n",
        "        if torch.max(captions) >= self.vocab_size:\n",
        "          print(\"ðŸš¨ Invalid token detected in captions!\")\n",
        "          print(\"Max token index:\", torch.max(captions).item(), \">= vocab_size:\", self.vocab_size)\n",
        "          print(\"Problematic captions:\", captions)\n",
        "          raise ValueError(\"Token index out of range for embedding.\")\n",
        "\n",
        "        embeddings = self.embedding(captions)\n",
        "        h, c = self.init_hidden_state(encoder_out.mean(dim=1))\n",
        "        outputs = torch.zeros(batch_size, max_len, self.vocab_size).to(captions.device)\n",
        "\n",
        "        for t in range(max_len):\n",
        "            context, _ = self.attention(encoder_out, h)\n",
        "            lstm_input = torch.cat([embeddings[:, t, :], context], dim=1)\n",
        "            h, c = self.decode_step(lstm_input, (h, c))\n",
        "            preds = self.fc(self.dropout_layer(h))\n",
        "            outputs[:, t, :] = preds\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def init_hidden_state(self, mean_encoder_out):\n",
        "        h = self.init_h(mean_encoder_out)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === WRAPPER ===\n",
        "class CaptioningModel(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(CaptioningModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        encoder_out = self.encoder(images)\n",
        "        outputs = self.decoder(encoder_out, captions)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "l5NxqrRSeRzl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GxEl1hYKXW_k"
      },
      "outputs": [],
      "source": [
        "# === TRAINING LOOP ===\n",
        "def train_model(model, train_dataset, val_dataset, word2idx, device='cuda',\n",
        "                batch_size=32, epochs=20, patience=3, lr=1e-4):\n",
        "    pad_idx = word2idx['<pad>']\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        tqdm_train = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")\n",
        "        for images, captions, _ in tqdm_train:\n",
        "            images, captions = images.to(device), captions.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, captions[:, :-1])\n",
        "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), captions[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            tqdm_train.set_postfix(loss=np.mean(train_losses))\n",
        "\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "\n",
        "        # === VALIDATION ===\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        references = []\n",
        "        hypotheses = []\n",
        "\n",
        "        tqdm_val = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\")\n",
        "        with torch.no_grad():\n",
        "            for images, captions, _ in tqdm_val:\n",
        "                images, captions = images.to(device), captions.to(device)\n",
        "                outputs = model(images, captions[:, :-1])\n",
        "                loss = criterion(outputs.reshape(-1, outputs.size(-1)), captions[:, 1:].reshape(-1))\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "                preds = torch.argmax(outputs, dim=2)\n",
        "                for ref, pred in zip(captions, preds):\n",
        "                    ref_tokens = [w for w in ref.tolist() if w not in {pad_idx, word2idx['<start>'], word2idx['<end>']}]\n",
        "                    pred_tokens = [w for w in pred.tolist() if w not in {pad_idx, word2idx['<start>'], word2idx['<end>']}]\n",
        "                    references.append([ref_tokens])\n",
        "                    hypotheses.append(pred_tokens)\n",
        "\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
        "        bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
        "        bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
        "        bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
        "        print(f\"BLEU-1 = {bleu1:.4f}, BLEU-2 = {bleu2:.4f}, BLEU-3 = {bleu3:.4f}, BLEU-4 = {bleu4:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"best_model_densenet.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aYwmged0gXDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c077ee-969e-413d-ed8e-0dfcf6aed271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# === DEVICE SETUP ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCnjMCOVYwso",
        "outputId": "edfb260e-74db-4748-eae1-8df898d2bc0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 22 02:44:54 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   56C    P8             18W /   72W |       3MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NLmNKL7Jgb-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f0072f-27f8-4b3e-a537-4dacc131ccd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# === DEVICE SETUP ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === BUILD AND TRAIN MODEL ===\n",
        "embed_dim = 256\n",
        "hidden_dim = 512\n",
        "attention_dim = 256\n",
        "dropout = 0.5\n",
        "#vocab_size = len(word2idx) + 1\n",
        "vocab_size = max(word2idx.values()) + 2  # allows for max token ID 2989\n",
        "\n",
        "encoder = DenseNetEncoder(encoded_image_size=7, fine_tune=True)\n",
        "decoder = DecoderRNNWithAttention(attention_dim, embed_dim, hidden_dim, vocab_size, encoder_dim=1024, dropout=dropout)\n",
        "model = nn.Sequential()  # Dummy fix to avoid Colab bug\n",
        "model = CaptioningModel(encoder, decoder).to(device)\n",
        "\n",
        "trained_model = train_model(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    word2idx=word2idx,\n",
        "    device=device,\n",
        "    batch_size=8,\n",
        "    epochs=20,\n",
        "    patience=3,\n",
        "    lr=1e-4\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQetLPXVdsZ2",
        "outputId": "3f5b262b-df49-436e-d735-edc90c9c9b24"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30.8M/30.8M [00:00<00:00, 185MB/s]\n",
            "Epoch 1 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [1:28:34<00:00,  1.42s/it, loss=4.15]\n",
            "Epoch 1 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [13:23<00:00,  1.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: Train Loss = 4.1511, Val Loss = 3.5983\n",
            "BLEU-1 = 0.3358, BLEU-2 = 0.1771, BLEU-3 = 0.0963, BLEU-4 = 0.0522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:24<00:00,  3.81it/s, loss=3.41]\n",
            "Epoch 2 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [01:15<00:00,  8.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: Train Loss = 3.4084, Val Loss = 3.2914\n",
            "BLEU-1 = 0.3568, BLEU-2 = 0.1988, BLEU-3 = 0.1144, BLEU-4 = 0.0643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:23<00:00,  3.81it/s, loss=3.13]\n",
            "Epoch 3 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [01:15<00:00,  8.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: Train Loss = 3.1274, Val Loss = 3.1383\n",
            "BLEU-1 = 0.3744, BLEU-2 = 0.2108, BLEU-3 = 0.1229, BLEU-4 = 0.0697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:23<00:00,  3.81it/s, loss=2.95]\n",
            "Epoch 4 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [01:15<00:00,  8.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: Train Loss = 2.9469, Val Loss = 3.0448\n",
            "BLEU-1 = 0.3529, BLEU-2 = 0.1985, BLEU-3 = 0.1165, BLEU-4 = 0.0662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:23<00:00,  3.81it/s, loss=2.81]\n",
            "Epoch 5 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [01:16<00:00,  8.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5: Train Loss = 2.8103, Val Loss = 2.9901\n",
            "BLEU-1 = 0.3744, BLEU-2 = 0.2128, BLEU-3 = 0.1258, BLEU-4 = 0.0721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:23<00:00,  3.81it/s, loss=2.7]\n",
            "Epoch 6 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [01:15<00:00,  8.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6: Train Loss = 2.6995, Val Loss = 2.9448\n",
            "BLEU-1 = 0.3732, BLEU-2 = 0.2118, BLEU-3 = 0.1245, BLEU-4 = 0.0704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:23<00:00,  3.81it/s, loss=2.61]\n",
            "Epoch 7 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [01:15<00:00,  8.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7: Train Loss = 2.6062, Val Loss = 2.9235\n",
            "BLEU-1 = 0.3943, BLEU-2 = 0.2269, BLEU-3 = 0.1344, BLEU-4 = 0.0767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:24<00:00,  3.81it/s, loss=2.52]\n",
            "Epoch 8 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [01:16<00:00,  8.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8: Train Loss = 2.5229, Val Loss = 2.8999\n",
            "BLEU-1 = 0.3531, BLEU-2 = 0.2003, BLEU-3 = 0.1174, BLEU-4 = 0.0658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:25<00:00,  3.81it/s, loss=2.45]\n",
            "Epoch 9 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [01:15<00:00,  8.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9: Train Loss = 2.4516, Val Loss = 2.8871\n",
            "BLEU-1 = 0.2992, BLEU-2 = 0.1702, BLEU-3 = 0.1003, BLEU-4 = 0.0565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:23<00:00,  3.81it/s, loss=2.39]\n",
            "Epoch 10 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [01:15<00:00,  8.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10: Train Loss = 2.3863, Val Loss = 2.8712\n",
            "BLEU-1 = 0.2923, BLEU-2 = 0.1664, BLEU-3 = 0.0988, BLEU-4 = 0.0561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:22<00:00,  3.82it/s, loss=2.32]\n",
            "Epoch 11 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [01:15<00:00,  8.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11: Train Loss = 2.3228, Val Loss = 2.8702\n",
            "BLEU-1 = 0.3591, BLEU-2 = 0.2059, BLEU-3 = 0.1230, BLEU-4 = 0.0708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:22<00:00,  3.82it/s, loss=2.27]\n",
            "Epoch 12 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [01:15<00:00,  8.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12: Train Loss = 2.2698, Val Loss = 2.8585\n",
            "BLEU-1 = 0.3176, BLEU-2 = 0.1815, BLEU-3 = 0.1081, BLEU-4 = 0.0624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:22<00:00,  3.82it/s, loss=2.22]\n",
            "Epoch 13 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [01:15<00:00,  8.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13: Train Loss = 2.2172, Val Loss = 2.8639\n",
            "BLEU-1 = 0.3367, BLEU-2 = 0.1924, BLEU-3 = 0.1143, BLEU-4 = 0.0657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:22<00:00,  3.82it/s, loss=2.17]\n",
            "Epoch 14 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [01:15<00:00,  8.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14: Train Loss = 2.1698, Val Loss = 2.8620\n",
            "BLEU-1 = 0.2942, BLEU-2 = 0.1679, BLEU-3 = 0.1003, BLEU-4 = 0.0578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:22<00:00,  3.82it/s, loss=2.12]\n",
            "Epoch 15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [01:15<00:00,  8.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15: Train Loss = 2.1242, Val Loss = 2.8593\n",
            "BLEU-1 = 0.3351, BLEU-2 = 0.1924, BLEU-3 = 0.1148, BLEU-4 = 0.0662\n",
            "Early stopping triggered at epoch 15\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qBx_DYuWgl4Y"
      },
      "outputs": [],
      "source": [
        "# === SAVE FINAL MODEL ===\n",
        "torch.save(model.state_dict(), os.path.join(base_path, \"data/roy_densenet_model.pt\"))\n",
        "torch.save(model, os.path.join(base_path, \"data/roy_densenet_model_full.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Max token ID in dataset:\", max(max(seq) for seqs in image_caption_seqs.values() for seq in seqs))\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2SUJf163p9L",
        "outputId": "5f108721-ede0-4a70-cecb-844468b33cea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max token ID in dataset: 2989\n",
            "Vocab size: 2991\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}